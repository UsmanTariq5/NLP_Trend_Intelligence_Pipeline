# TrendScope Analytics - NLP Trend Intelligence Pipeline

## ğŸ“‹ Project Overview

A reproducible NLP data engineering pipeline that collects, processes, and analyzes emerging technology products to identify linguistic trends and patterns. This project demonstrates end-to-end data pipeline development with version control, text processing, and automated orchestration.

**Business Goal**: Track emerging tech products, identify trending themes, analyze linguistic evolution, and maintain reproducible, versioned datasets.

## ğŸ—ï¸ Architecture

```
Data Acquisition â†’ Text Processing â†’ Feature Engineering â†’ Analysis â†’ Versioning
     (Scraper)      (Preprocessing)    (BoW, N-grams)    (Statistics)    (DVC)
```

## ğŸ“Š Pipeline Stages

### Stage 1: Data Acquisition
- **Source**: GitHub Trending Repositories (as proxy for tech products)
- **Target**: 300+ product listings
- **Features**:
  - Rate limiting (1.2s between requests)
  - Retry logic (3 attempts with exponential backoff)
  - Missing value handling
- **Output**: `data/raw/products_raw.json`

### Stage 2: Data Versioning
- **Tool**: DVC (Data Version Control)
- **Remote**: DagsHub S3-compatible storage
- **Tracked Assets**:
  - Raw datasets
  - Processed datasets
  - Feature representations
- **Versioning**: Multiple dataset versions (v1: 300 entries)

### Stage 3: Text Processing
- **Preprocessing Pipeline**:
  1. Unicode normalization
  2. HTML removal
  3. URL removal
  4. Lowercasing
  5. Punctuation removal
  6. Tokenization
  7. Stopword removal
  8. Lemmatization
  9. Token filtering (length â‰¥ 2, non-numeric)
- **Output**: `data/processed/products_clean.csv`

### Stage 4: Data Representation
- **Manual Implementations**:
  - Vocabulary extraction (2,455 unique tokens)
  - One-Hot Encoding
  - Bag-of-Words matrix (300 Ã— 2,455)
  - Unigram frequency distribution
  - Bigram frequency distribution
- **Outputs**:
  - `data/features/vocab.json`
  - `data/features/bow_matrix.npy`
  - `data/features/onehot_sample.npy`
  - `data/features/ngram_frequencies.json`

### Stage 5: Linguistic Intelligence
- **Statistical Analysis**:
  - Top 30 unigrams
  - Top 20 bigrams
  - Most common tags/categories
  - Vocabulary size: 2,455 tokens
  - Average description length: 33.94 tokens
  - Duplicate detection using Minimum Edit Distance
  - Unigram probability estimation
  - Perplexity calculation on 5 held-out descriptions
- **Output**: `reports/trend_summary.txt`

### Stage 6: Airflow Pipeline Orchestration
- **DAG Tasks**:
  1. `scrape_data` - Data acquisition
  2. `preprocess_data` - Text cleaning
  3. `generate_features` - Feature engineering
  4. `compute_statistics` - Analysis
  5. `dvc_push` - Version control
- **Features**:
  - Automatic retries (2 attempts)
  - Task dependencies
  - Logging
  - Manual triggering support
- **Schedule**: Weekly execution

## ğŸ“ Project Structure

```
trend_intelligence_pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ nlp_trend_dag.py          # Airflow DAG definition
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py                # Data acquisition module
â”‚   â”œâ”€â”€ preprocess.py             # Text preprocessing module
â”‚   â”œâ”€â”€ representation.py         # Feature engineering module
â”‚   â””â”€â”€ statistics.py             # Statistical analysis module
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ products_raw.json     # Raw scraped data
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ products_clean.csv    # Cleaned data
â”‚   â””â”€â”€ features/
â”‚       â”œâ”€â”€ vocab.json            # Vocabulary
â”‚       â”œâ”€â”€ bow_matrix.npy        # Bag-of-Words matrix
â”‚       â”œâ”€â”€ onehot_sample.npy     # One-Hot encoding sample
â”‚       â””â”€â”€ ngram_frequencies.json # N-gram frequencies
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ trend_summary.txt         # Linguistic intelligence report
â”‚
â”œâ”€â”€ dvc.yaml                      # DVC pipeline configuration
â”œâ”€â”€ .dvc/                         # DVC internal files
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- Git
- DVC (Data Version Control)
- Apache Airflow (for orchestration)

### Installation

1. **Clone Repository**:
   ```bash
   git clone <repository-url>
   cd trend_intelligence_pipeline
   ```

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Initialize DVC**:
   ```bash
   dvc init
   ```

4. **Configure DVC Remote** (Optional - DagsHub):
   ```bash
   dvc remote add -d dagshub dagshub://your-username/your-repo
   dvc remote modify dagshub --local auth basic
   dvc remote modify dagshub --local user your-username
   dvc remote modify dagshub --local password your-token
   ```

### Running the Pipeline

#### Option 1: Run Individual Stages
```bash
# Stage 1: Data Acquisition
python src/scraper.py

# Stage 2: Text Preprocessing
python src/preprocess.py

# Stage 3: Feature Engineering
python src/representation.py

# Stage 4: Statistical Analysis
python src/statistics.py
```

#### Option 2: Run with Airflow
```bash
# Initialize Airflow
airflow db init

# Copy DAG to Airflow directory
cp dags/nlp_trend_dag.py $AIRFLOW_HOME/dags/

# Start Airflow webserver
airflow webserver -p 8080

# Start Airflow scheduler (in separate terminal)
airflow scheduler

# Trigger DAG manually
airflow dags trigger nlp_trend_intelligence_pipeline
```

## ğŸ“ˆ Key Results

### Dataset Statistics
- **Total Products**: 300
- **Vocabulary Size**: 2,455 unique tokens
- **Total Tokens**: 10,182
- **Average Description Length**: 33.94 tokens
- **Matrix Sparsity**: 99.20%

### Top Trending Terms
1. **machine** - 453 occurrences
2. **learn** - 387 occurrences
3. **model** - 321 occurrences
4. **data** - 298 occurrences
5. **tool** - 276 occurrences

### Language Model Performance
- **Average Perplexity**: 1,237.55
- Lower perplexity indicates better model fit to the data

### Duplicate Detection
- **Potential Duplicates**: 166 pairs (using edit distance â‰¤ 3)

## ğŸ”¬ Technical Implementation Details

### Manual NLP Implementations

1. **Vocabulary Building**:
   - Token frequency counting
   - Sorted by frequency and alphabetically
   - Word-to-index mapping

2. **Bag-of-Words**:
   - Manual sparse matrix construction
   - Frequency-based representation
   - Shape: (300 documents, 2,455 features)

3. **N-gram Extraction**:
   - Unigram frequencies: 2,455 unique
   - Bigram frequencies: 5,567 unique
   - Manual sliding window implementation

4. **Minimum Edit Distance**:
   - Dynamic programming algorithm
   - O(n*m) time complexity
   - Used for duplicate detection

5. **Language Model**:
   - Unigram model with Laplace smoothing
   - Probability estimation: P(w) = (count(w) + 1) / (N + V)
   - Perplexity: 2^(-1/N * Î£ logâ‚‚(P(w)))

## ğŸ”„ Data Versioning with DVC

### Tracking Datasets
```bash
dvc add data/raw/products_raw.json
dvc add data/processed/products_clean.csv
dvc add data/features/vocab.json
dvc add data/features/bow_matrix.npy
```

### Committing Changes
```bash
git add data/raw/.gitignore data/raw/products_raw.json.dvc
git commit -m "Add raw dataset v1"
```

### Pushing to Remote
```bash
dvc push
```

### Retrieving Specific Version
```bash
git checkout <commit-hash>
dvc checkout
```

## ğŸ› ï¸ Reproducibility

This pipeline ensures reproducibility through:
1. **Version Control**: Git for code, DVC for data
2. **Dependency Management**: `requirements.txt` with pinned versions
3. **Modular Design**: Independent, reusable components
4. **Documentation**: Comprehensive inline comments
5. **Automated Orchestration**: Airflow DAG for consistent execution

## ğŸ“ NLP Concepts Demonstrated

- âœ… Text Preprocessing
- âœ… Tokenization
- âœ… Stopword Removal
- âœ… Lemmatization
- âœ… Vocabulary Extraction
- âœ… One-Hot Encoding
- âœ… Bag-of-Words
- âœ… N-gram Analysis (Unigrams, Bigrams)
- âœ… Minimum Edit Distance
- âœ… Language Model (Unigram)
- âœ… Perplexity Calculation

## ğŸ¤ Contributing

This is an academic project for NLP coursework. For suggestions or improvements:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is created for educational purposes as part of NLP coursework.

## ğŸ‘¥ Authors

- **TrendScope Analytics Team**
- NLP Research & Innovation Department

## ğŸ™ Acknowledgments

- GitHub API for data access
- Open-source NLP community
- Course instructors and teaching assistants

---

**Note**: This pipeline focuses on data engineering and linguistic analysis. No machine learning model training is included as per project requirements.

## ğŸ“ Support

For questions or issues:
- Open an issue in the repository
- Contact: analytics@trendscope.com
- Documentation: See inline code comments

---

*Last Updated: February 28, 2026*
